---
title: "SMPE Project"
author: "Insaf Boukrouh & Othmane Nahyl"
date: "January 2017"
output:
  pdf_document:
    fig_height: 7
    fig_width: 10
    highlight: tango    
  html_document:
    fig_height: 7
    fig_width: 10
    highlight: tango
---

Airbnb is a quickly evolving marketplace that allows people to list, discover, and book unique accommodations around the world.

For the "Scientific Methodology and Performance Evaluation" project, we chose a dataset that contains information about Airbnb listings in the United States.

The dataset is available [here](http://insideairbnb.com/get-the-data.html).

The first thing to do is to load and clean the dataset.

# Loading the dataset
```{r, message=F, warning=F, cache=T}
library(plyr)
library(doParallel)
library(ggplot2)
library(grid)
library(gridExtra)
library(maps)

# Function to remove lines with NA values in the columns that we're interested in
removeRowsWithNA <- function(data, desiredCols) {
  completeVec <- complete.cases(data[, desiredCols])
  return(data[completeVec, ])
}

# Function to remove spaces
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# Register parallel backend in order to load files in parallel
nodes <- detectCores()
cl <- makeCluster(nodes)
registerDoParallel(cl)

# Load the csv files and keep certain columns
whole_data = ldply(list.files(path="../usa-data/",pattern="csv",full.names=TRUE),.parallel = TRUE,function(filename) {
    variables_to_keep <- c("id","host_id","host_response_time","host_response_rate","host_acceptance_rate","host_total_listings_count","zipcode","country_code","property_type","room_type","accommodates","bathrooms","bedrooms","beds","bed_type","square_feet","price","security_deposit","cleaning_fee","extra_people","minimum_nights","maximum_nights","number_of_reviews","review_scores_rating","review_scores_accuracy","review_scores_cleanliness","review_scores_checkin","review_scores_communication","review_scores_location","review_scores_value","instant_bookable","cancellation_policy","host_since","calendar_last_scraped","city","state")
    city_data = read.csv(filename)
    city_data <- city_data[variables_to_keep]
    
    return(city_data)
})

stopCluster(cl)
```

# Cleaning the dataset
```{r, message=F, warning=F, cache=T}
# Convert host_response_rate from factors to numeric values
whole_data$host_response_rate <- as.numeric(gsub("%", "", as.character(whole_data$host_response_rate)))/100

# Convert host_acceptance_rate from factors to numeric values
whole_data$host_acceptance_rate <- as.numeric(gsub("%", "", as.character(whole_data$host_acceptance_rate)))/100

# Convert price from factors to numeric values
whole_data$price <- as.double(substring(gsub(",", "", as.character(whole_data$price)),2))

# Convert security_deposit from factors to numeric values
whole_data$security_deposit <- as.double(substring(gsub(",", "", as.character(whole_data$security_deposit)),2))
whole_data$security_deposit[is.na(whole_data$security_deposit)] <- 0

# Convert cleaning_fee from factors to numeric values
whole_data$cleaning_fee <- as.double(substring(gsub(",", "", as.character(whole_data$cleaning_fee)),2))
whole_data$cleaning_fee[is.na(whole_data$cleaning_fee)] <- 0

# Convert extra_people from factors to numeric values
whole_data$extra_people <- as.double(substring(gsub(",", "", as.character(whole_data$extra_people)),2))
whole_data$extra_people[is.na(whole_data$extra_people)] <- 0

# Clean and correct the states names
whole_data <- removeRowsWithNA(whole_data, c("price","state"))
whole_data$state <- trim(whole_data$state)
whole_data <- subset(whole_data, whole_data$state != '')
whole_data$state[whole_data$state == 'New York'] <- 'NY'
whole_data$state[whole_data$state == 'ny'] <- 'NY'
whole_data$state[whole_data$state == 'Baja California'] <- 'CA'
whole_data$state[whole_data$state == 'secc Terrazas'] <- 'CA'
whole_data$state[whole_data$state == 'ca'] <- 'CA'
whole_data$state[whole_data$state == 'Ca'] <- 'CA'
whole_data$state[whole_data$state == 'wa'] <- 'WA'
whole_data$state[whole_data$state == 'il'] <- 'IL'
whole_data$state[whole_data$state == 'Il'] <- 'IL'
whole_data <- subset(whole_data, whole_data$state != 'MP')
whole_data <- subset(whole_data, whole_data$state != 'VT')
```

# Miscellaneous

In order to better understand the dataset we can make things visual. Basically we can generate plots and look at them before starting to ask questions.

## Number of listings per state
```{r, message=F, warning=F, cache=T}
# Replace the abbreviations of the states by the full names
usa_data <- whole_data
usa_data$state[usa_data$state == 'CA'] <- 'california'
usa_data$state[usa_data$state == 'DC'] <- 'district of columbia'
usa_data$state[usa_data$state == 'NY'] <- 'new york'
usa_data$state[usa_data$state == 'TN'] <- 'tennessee'
usa_data$state[usa_data$state == 'LA'] <- 'louisiana'
usa_data$state[usa_data$state == 'NJ'] <- 'new jersey'
usa_data$state[usa_data$state == 'WA'] <- 'washington'
usa_data$state[usa_data$state == 'MD'] <- 'maryland'
usa_data$state[usa_data$state == 'NC'] <- 'north carolina'
usa_data$state[usa_data$state == 'OR'] <- 'oregon'
usa_data$state[usa_data$state == 'TX'] <- 'texas'
usa_data$state[usa_data$state == 'MA'] <- 'massachusetts'
usa_data$state[usa_data$state == 'IL'] <- 'illinois'
usa_data$state[usa_data$state == 'CO'] <- 'colorado'

# Get the number of listings per state
number_of_listings_by_state <- aggregate(cbind(usa_data$id), by = list(state = usa_data$state), FUN = length)

# Fill missing rows with dummy states avoid some ggplot errors
for (i in 17:49){
  df <- data.frame(paste("state",i),0)
  names(df) <- c("state","V1")
  number_of_listings_by_state <- rbind(df, number_of_listings_by_state)
}

# Prepare data for ggplot
states_map <- map_data("state")
states_and_positions <- aggregate(cbind(long, lat) ~ region, data = states_map, FUN = function(x) mean(range(x)))
colnames(states_and_positions)[1] <- "state"
plot_data <- merge(number_of_listings_by_state, states_and_positions, by = "state", all.x = TRUE)

# Plot the map
ggplot(number_of_listings_by_state, aes(map_id = number_of_listings_by_state$state)) +
    geom_map(aes(fill = number_of_listings_by_state$V1), map = states_map) +
    scale_fill_continuous("Number of listings", low='coral', high='coral4', guide='colorbar') + 
    expand_limits(x = states_map$long, y = states_map$lat) +
    geom_text(data = plot_data, aes(long, lat, label = plot_data$state), size=2) +
    labs(title = "Listings in the USA", x = "", y = "") +
    theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank())
```

From the plot, we can see that we only have data for some states (to be exact, 14 out of 50).
We can see that California and New York have the highest number of listings. 
Thus, any conclusion that we make through this study can't be generalized over the whole country.

## Incomes
```{r, message=F, warning=F, cache=T}
# Clean
incomes_data <- removeRowsWithNA(whole_data, c("price", "minimum_nights", "number_of_reviews", "host_since", "calendar_last_scraped"))

# Calculate the income for each listing
incomes_data$income <- incomes_data$number_of_reviews*incomes_data$minimum_nights*incomes_data$price

# This duration should be the same for listings having the same host 
incomes_data$duration <- abs(as.Date(as.character(incomes_data$calendar_last_scraped), format = "%Y-%m-%d") -
                          as.Date(as.character(incomes_data$host_since), format = "%Y-%m-%d"))/30

# Aggregate the income by host
incomes_data <- aggregate(
    cbind(incomes_data$income, incomes_data$duration),
    by = list(host_id = incomes_data$host_id),
    FUN = function(x) sum(x))

# Calculate the income per month
incomes_data$income_per_month = unlist(incomes_data['V1'])/unlist(incomes_data['V2'])

# Plot
ggplot(incomes_data, aes(x = income_per_month))+
  geom_histogram(color = "darkblue", fill = "lightblue", binwidth = 10)+
  labs(title = "Incomes in the USA", x = "Income per month", y = "Count")+
  coord_cartesian(xlim=c(0,500))
```

While a lot of people gain big amounts of money through Airbnb, most of them have an average income lower than 100 dollars per month.

However, this plot corresponds to the minimum income and doesn't exactly reflect the reality. In fact, to calculate the incomes we made the following hypotheses:
- The number of the reservations of every listing is the number of the reviews for that listing (statisically, the great majority - approximately 80% - of Airbnb guests leave reviews for the hosts).
- Every listing was reserved for a number of days that corresponds to the minimum booking duration.

## Type of listings
```{r, message=F, warning=F, cache=T}
# Get the room types and their percentages
room_types_counts <- table(whole_data$room_type)
room_types <- names(room_types_counts)
counts <- as.vector(room_types_counts)
percentages <- scales::percent(round(counts/sum(counts), 2))
room_types_percentages <- sprintf("%s (%s)", room_types, percentages)
room_types_counts_df <- data.frame(group = room_types, value = counts)

# Plot
pie <- ggplot(room_types_counts_df, aes(x = "", y = value, fill = room_types_percentages))+
  geom_bar(width = 1, stat = "identity")+
  coord_polar("y", start = 0)+
  scale_fill_brewer("Room Types", palette = "Dark2")+
  ggtitle("Type of listings")+
  ylab("")+
  xlab("")+
  labs(fill="")+
  theme(axis.ticks = element_blank(), panel.grid = element_blank(), axis.text = element_blank())+
  geom_text(aes(label = percentages), size = 5, position = position_stack(vjust = 0.5))
pie
```

Most of the listings are entire homes or apartments, while the shared rooms don't represent more than 3% of the entire listings.

## Type of properties
```{r, message=F, warning=F, cache=T}
# Get the property types and their percentages
whole_property_types_counts <- table(whole_data$property_type)
property_types_counts <- table(whole_data$property_type,exclude=names(whole_property_types_counts[whole_property_types_counts[] < 4000]))
count_of_others <- sum(as.vector(whole_property_types_counts[whole_property_types_counts[] < 4000]))
property_types_counts['Others'] <- count_of_others
property_types <- names(property_types_counts)
counts <- as.vector(property_types_counts)
percentages <- scales::percent(round(counts/sum(counts), 2))
property_types_percentages <- sprintf("%s (%s)", property_types, percentages)
property_types_counts_df <- data.frame(group = property_types, value = counts)

# Plot
pie <- ggplot(property_types_counts_df, aes(x = "", y = value, fill = property_types_percentages))+
  geom_bar(width = 1, stat = "identity")+
  coord_polar("y", start = 0)+
  scale_fill_brewer("Property Types", palette="Dark2")+
  ggtitle("Type of listings")+
  ylab("")+
  xlab("")+
  theme(axis.ticks = element_blank(), panel.grid = element_blank(), axis.text = element_blank())+
  geom_text(aes(label = percentages), size = 4, position = position_stack(vjust = 0.5))
pie
```


## Evolution of hosts over time
```{r, message=F, warning=F, cache=T}
# Clean
new_hosts_data <- removeRowsWithNA(whole_data, c("host_since"))

# Calculate the number of new hosts for each year (except for 2017 since our data is not complete for this year)
new_hosts_data$host_since <- as.Date(new_hosts_data$host_since, '%Y-%m-%d')
new_hosts_data <- new_hosts_data[new_hosts_data$host_since < as.Date("2017-01-01"),]
new_hosts_data <- new_hosts_data[order(as.Date(new_hosts_data$host_since, format="%Y-%m-%d")),]
new_hosts_data$host_since <- format(as.Date(new_hosts_data$host_since, "%Y-%m-%d"), format="%Y-%m")
new_hosts_data_table <- table(new_hosts_data$host_since)

# Plot
plot(as.Date(paste(format(names(new_hosts_data_table), format="%Y-%m"),"-01", sep="")), as.vector(new_hosts_data_table), type = "l", xlab = "Time", ylab = "Number of new hosts", col = "Blue")

plot(as.Date(paste(format(names(new_hosts_data_table), format="%Y-%m"),"-01", sep="")), cumsum(as.vector(new_hosts_data_table)), type = "l", xlab = "Time", ylab = "Number of hosts", col = "Red")
```

The number of new hosts was increasing since 2008. However, there was a decrease of this number in the last two years.

## Airbnb Prices
```{r, message=F, warning=F, cache=T}
# Calculate the average price per state
average_prices_per_state <- aggregate(cbind(whole_data$price),
                  by = list(state = whole_data$state),
                  FUN = function(x) mean(x))

# Plot
ggplot(data = average_prices_per_state, aes(x = average_prices_per_state$state, y = average_prices_per_state$V1))+
    geom_bar(stat = "identity", fill = "steelblue", width = 0.7)+
    geom_text(aes(label = round(average_prices_per_state$V1, 2)), size=4)+
    coord_flip()+
    xlab("State")+
    ylab("Average Daily Price")+  
    theme_minimal()
```

The average daily price of listings in the state of Texas is the highest (292.81 dollars), while it's the lowest in New Jersey (97.5 dollars). However, we can't conclude anything from the plot since the average prices alone aren't representative of the whole prices in every state.

To make a conclusion, we have to analyze the variability of the prices.
```{r, message=F, warning=F, cache=T}
ggplot(data = whole_data, aes(x = state, y = price, color = state)) +geom_boxplot(outlier.shape = NA) +theme(axis.text.x = element_text(angle = 90, hjust = 1))+coord_cartesian(ylim = c(0, 750))
```

The plot shows that we have a high variability of the prices in the majority of the states, especially in Texas. Thus, the plot of average daily prices was misleading.

## Price plots according to different parameters
```{r, message=F, warning=F, cache=T}
whole_data_without_null_host_response_time <-subset(whole_data,host_response_time != "N/A" & host_response_time != "")
ggplot(data = whole_data_without_null_host_response_time, aes(x = host_response_time, y = price,color=host_response_time)) +geom_boxplot(outlier.shape = NA) +theme(axis.text.x = element_text(angle = 90, hjust = 1))+coord_cartesian(ylim = c(0, 400))

ggplot(data = whole_data, aes(x = host_response_rate, y = price, color=host_response_rate)) +geom_point(size=0.1)

ggplot(data = whole_data, aes(x = host_total_listings_count, y = price, color=host_total_listings_count)) +geom_point(size=0.1) 

whole_data_without_null_property_type <-subset(whole_data,property_type != "N/A" & property_type != "")
ggplot(data = whole_data_without_null_property_type, aes(x = property_type, y = price,color=property_type)) +geom_boxplot(outlier.shape = NA) +theme(axis.text.x = element_text(angle = 90, hjust = 1)) +coord_cartesian(ylim = c(0, 1700))

ggplot(data = whole_data, aes(x = room_type, y = price,color=room_type)) +geom_boxplot(outlier.shape = NA) +theme(axis.text.x = element_text(angle = 90, hjust = 1)) +coord_cartesian(ylim = c(0, 500))

ggplot(data = whole_data, aes(x = accommodates, y = price,color=accommodates)) +geom_jitter(position=position_jitter(0.3),size=0.1) 

ggplot(data = whole_data, aes(x = bathrooms, y = price, color=bathrooms)) +geom_jitter(width = 0.1,height = 0.2,size=0.1) 

ggplot(data = whole_data, aes(x = bedrooms, y = price,color=bedrooms)) +geom_jitter(position=position_jitter(0.3),size=0.1) 

ggplot(data = whole_data, aes(x = beds, y = price,color=beds)) +geom_jitter(position=position_jitter(0.3),size=0.1)

ggplot(data = whole_data, aes(x = bed_type, y = price,color=bed_type)) +geom_boxplot(outlier.shape = NA) +coord_cartesian(ylim = c(0, 400))

ggplot(data = whole_data, aes(x = square_feet, y = price,color=square_feet)) +geom_point(size=0.1) +coord_cartesian(ylim = c(0, 7500))

ggplot(data = whole_data, aes(x = minimum_nights, y = price,color=minimum_nights))  +geom_point(size=0.1)

ggplot(data = whole_data, aes(x = maximum_nights, y = price,color=maximum_nights)) +geom_point(size=0.1)

ggplot(data = whole_data, aes(x = instant_bookable, y = price,color=instant_bookable)) +geom_boxplot(outlier.shape = NA) +coord_cartesian(ylim = c(0, 400))

ggplot(data = whole_data, aes(x = cancellation_policy, y = price,color=cancellation_policy)) +geom_boxplot(outlier.shape = NA) +theme(axis.text.x = element_text(angle = 90, hjust = 1)) +coord_cartesian(ylim = c(0, 3000))
```

The plots show no correlations between the price and the majority of the parameters: the host response time, the host response rate, the host total listings count, the number of accommodates, the number of bathrooms, the number of bedrooms, the number of beds, the number of minimum and maximum nights (most of the listings don't have restrictions about these two parameters), whether the listing is instant bookable or not, and the cancellation policy (we can only see that the prices are slightly more expensive for the listings that have a strict cancellation policy).

For the square_feet, the great majority of the listings don't have this information. Thus, we can't conclude anything regarding it.

There are possible correlations between the price and:
- The property type: the prices are very expensive for some types like the earth houses and the tree houses.
- The room type: the entire homes are generally more expensive than the other types, and the shared rooms are the cheapest ones.
- The bed type: beds other than real ones are generally cheaper, but we have a big variability for the real beds.

In terms of variability, it's high for most of the boxplots. We also see that most of them are skewed towards the bottom, which indicates that the higher the prices are, the more spread out they are.

## Different types of ratings
```{r, message=F, warning=F, cache=T}
scores <- c("review_scores_rating","review_scores_accuracy","review_scores_cleanliness","review_scores_checkin","review_scores_communication","review_scores_location","review_scores_value")
scores_data <- whole_data[scores]
scores_data <- removeRowsWithNA(scores_data, scores)

ggplot(data = scores_data, aes(x = review_scores_accuracy, y = review_scores_rating, color=review_scores_accuracy)) + geom_jitter(size = 0.1)

ggplot(data = scores_data, aes(x = review_scores_cleanliness, y = review_scores_rating, color=review_scores_cleanliness)) + geom_jitter(size = 0.1)

ggplot(data = scores_data, aes(x = review_scores_checkin, y = review_scores_rating, color=review_scores_checkin)) + geom_jitter(size = 0.1)

ggplot(data = scores_data, aes(x = review_scores_communication, y = review_scores_rating, color=review_scores_communication)) + geom_jitter(size = 0.1)

ggplot(data = scores_data, aes(x = review_scores_location, y = review_scores_rating, color=review_scores_location)) + geom_jitter(size = 0.1)

ggplot(data = scores_data, aes(x = review_scores_value, y = review_scores_rating, color=review_scores_value)) + geom_jitter(size = 0.1)
```

From the plots, we can see that most of the people who give the listings high ratings, give high scores for all the other types of scores.


# Regression to explain the Ratings
Now that we have taken a look at our data, one question arises: What factors do users take into consideration to globally rate a listing ?

## Trying to fit a linear model to explain the Ratings
```{r, message=F, warning=F, cache=T}
# Linear model with all variables
r_reg <- lm(data=whole_data, review_scores_rating~review_scores_accuracy+review_scores_cleanliness+review_scores_checkin+review_scores_communication+review_scores_location+review_scores_value)
summary(r_reg)
```
Our model has:

- A fairly good Adjusted R-squared value (0.7304).

- A good p-value (< 2.2e-16).

- All the predictors are significant.

However, this is not enough to say that our linear model is good. To make sure of that, let's check whether the linear regression hypothesis hold or not.
```{r, message=F, warning=F, cache=T}
plot(r_reg)
```

- The "Residuals vs Fitted" plot indicates if residuals have non-linear patterns. If the residuals are spread out around a horizontal line without distinct patterns, that means that we don't have non-linear relationships, which is not the case for our model since it seems that the residuals have some kind of structure.

- The "Normal Q-Q" plot shows if residuals are normally distributed. Our residuals are not well lined on the straight dashed line except in the middle of the plot, which is not quite good.

- The "Scale-Location" plot lets us check the assumption of equal variance (homoscedasticity). Our line is not horizontal with randomly spread points, thus, our residuals are not homoscedastic. This was expected, since from the previous plots of the different types of ratings, we could clearly see that the variance depends on the score.

- The "Residuals vs Leverage" plot helps us find influential cases. In fact, even though data has outliers, they might not be influential to determine a regression line. In our plot, we can barely see Cook's distance lines because all cases are well inside of them. i.e: if we exclude the "52474" case for example, the changes in the slope coefficients won't be important.

To conclude, our linear model doesn't well explain the ratings.


# Regression to explain the Price

Another important question that arises is: Knowing the listing's attributes, can we predict its price ?

## Trying to fit a linear model to explain the Price
```{r, message=F, warning=F, cache=T}
# We split our data to a training set and a test set
smp_size <- floor(0.9 * nrow(whole_data))
set.seed(357)
train_ind <- sample(seq_len(nrow(whole_data)), size = smp_size)
train <- whole_data[train_ind, ]
test <- whole_data[-train_ind, ]
```

```{r, message=F, warning=F, cache=T}
# We try to fit a model with all the variables
p_reg1 <- lm(data=train, price~host_response_rate+host_acceptance_rate+host_total_listings_count+zipcode+property_type+room_type+accommodates+bathrooms+bedrooms+beds+bed_type+square_feet+security_deposit+cleaning_fee+extra_people+minimum_nights+maximum_nights+number_of_reviews+instant_bookable+cancellation_policy+state)
summary(p_reg1)
```

For this model, we have a very good value of the adjusted R-Squared (0.8115), however, 129338 observations were deleted due to missingness, which leaves us only with 14608 observations.

Let's try to fit another model without the explanatory variables that are missing from most of the observations.
```{r, message=F, warning=F, cache=T}
p_reg2 <- lm(data=train, price~zipcode+property_type+room_type+accommodates+bathrooms+bedrooms+beds+bed_type+instant_bookable+cancellation_policy+state)
summary(p_reg2)
```
For this model, we have:

- A good p-value.

- But a very low value of the adjusted R-squared (0.2632).

Let's try to see if we have some outliers that have a big impact on our coeffiecients.
For that, we have to plot the Residuals versus Leverage plot.
```{r, message=F, warning=F, cache=T}
plot(p_reg2)
```

We see that there are some observations outside the dashed red lines, let's try to inspect some of them.
```{r, message=F, warning=F, cache=T}
train$rowid <- row.names(train)
train[train$rowid == 114438,]$price
train[train$rowid == 61737,]$price
```

We can conclude that listings with very high prices influence our model because they increase the variability of the price.

```{r, message=F, warning=F, cache=T}
nrow(train[train$price > 1000,])/nrow(train)  
```

From this we can see that listings with prices higher than 1000 dollars represent approximately 1% of our data.

Let's try to fit a model after removing those listings.
```{r, message=F, warning=F, cache=T}
sub_data <- whole_data[whole_data$price <= 1000,]
smp_size <- floor(0.9 * nrow(sub_data))
set.seed(357)
train_ind <- sample(seq_len(nrow(sub_data)), size = smp_size)
train_sub <- sub_data[train_ind, ]
test_sub <- sub_data[-train_ind, ]

p_reg3 <- lm(data=train_sub, price~zipcode+property_type+room_type+accommodates+bathrooms+bedrooms+beds+bed_type+instant_bookable+cancellation_policy+state)
summary(p_reg3)
```

Our model is better than the previous one in terms of the adjusted R-squared (0.4379), but it's still not a good one.

The following graph is a plot with x-axis as the actual prices and y-axis as the predicted ones 
Ideally, all the points should be close to a diagonal line, which is obviously not the case here.
```{r}
predictions <- predict(p_reg3, test_sub)
plot(predictions, test_sub$price, xlab="predicted", ylab="actual")
abline(a=0, b=1, col="blue")
```

In conclusion, there could be many reasons to explain these bad results, for example: 

- Our dataset can't be explained using a linear model.

- Our dataset lacks a lot of data that surely has an impact on the price, such as the surface.

- Our study doesn't take into consideration seasonality: the prices may depend on time.